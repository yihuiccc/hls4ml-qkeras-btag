{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8e79411",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aca9dbab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-11 12:31:18.989824: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-08-11 12:31:19.017006: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-08-11 12:31:19.017031: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1759fc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"./Jan06_FlavFix_smear_1_std_xtd_zst.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c4ab3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "f5 = h5py.File(file_path, 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d615c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array( f5['x_train'] )\n",
    "y_train = to_categorical ( np.array( f5['y_train'] ) )\n",
    "w_train = np.array( f5['w_train'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cc112674",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[400610.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
       "              0.,      0., 300906.],\n",
       "        [601755.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
       "              0.,      0.,  99761.],\n",
       "        [400667.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
       "              0.,      0., 300849.]]),\n",
       " array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ],\n",
       "       dtype=float32),\n",
       " <a list of 3 BarContainer objects>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# x_train = x_train[:,:15,:]\n",
    "#print(x_train.shape)\n",
    "#print(y_train[15,1])\n",
    "plt.hist(y_train[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4a771221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x-shape:  (701516, 15, 6)\n",
      "y-shape:  (701516, 3)\n",
      "w-shape:  (701516,)\n"
     ]
    }
   ],
   "source": [
    "print(\"x-shape: \", x_train.shape)\n",
    "print(\"y-shape: \", y_train.shape)\n",
    "print(\"w-shape: \", w_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ee1934b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-1.90562449e-01 -1.51768191e-02 -3.60630939e-01  4.87624739e-03\n",
      "    7.29672852e-01 -9.82183589e-01]\n",
      "  [-1.90693684e-01 -1.91545118e-03 -5.80833999e-01  4.31781846e-02\n",
      "    1.22056850e-01 -4.08898653e-01]\n",
      "  [ 5.29769396e-02  1.65169640e-02  4.14670362e-01  2.45462729e-02\n",
      "   -6.13277289e-01  6.71777033e-01]\n",
      "  ...\n",
      "  [ 2.33517491e-02  2.63244143e+00  1.74061825e-01  2.58601275e+00\n",
      "   -8.02894387e-01  9.94843555e-01]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00]]\n",
      "\n",
      " [[ 1.02449184e+00  5.74589139e-01  1.32411625e+00  4.22748161e-01\n",
      "   -1.53972043e+00  9.63009351e-01]\n",
      "  [ 1.69704721e-01  2.56993213e+00  1.59799224e+00  4.42930636e+00\n",
      "   -1.12664531e+00 -6.42693874e-01]\n",
      "  [-4.65999918e-02 -1.18592952e+00  1.72322546e-01 -1.91137920e+00\n",
      "   -1.29164736e+00 -7.46956055e-02]\n",
      "  ...\n",
      "  [-2.10206523e-01 -2.71033332e+00 -8.97439126e-01 -2.63426587e+00\n",
      "   -1.60214147e+00  4.70275867e-01]\n",
      "  [-1.70065272e-01 -4.37339147e-03 -8.87704767e-01 -1.64784798e-03\n",
      "   -8.64556976e-01  2.43852097e-01]\n",
      "  [ 1.35420453e-01 -2.56575826e+00  8.27441098e-01 -2.57519999e+00\n",
      "   -1.67382364e+00  1.01431727e+00]]\n",
      "\n",
      " [[ 5.41934260e-01 -1.86391642e-02  6.67541354e-02 -7.20812961e-03\n",
      "   -1.96914338e-01  7.00119024e-01]\n",
      "  [ 7.27431327e-01  5.87918816e-01  1.21572804e-01  1.60556809e-03\n",
      "    1.89233017e+00 -3.73306488e-01]\n",
      "  [-2.10337583e-01 -3.79737314e-02 -7.30128662e-01 -7.90670526e-03\n",
      "    2.01337872e+00 -1.07204963e+00]\n",
      "  ...\n",
      "  [ 1.74568636e-02 -5.28663600e-01  7.59034330e-02 -8.61345205e-02\n",
      "    1.89929178e-01  3.62429827e-01]\n",
      "  [ 2.95754419e-02 -2.75799924e-01  7.14255083e-02 -2.90185479e-02\n",
      "   -7.29256463e-02  1.59640766e-01]\n",
      "  [-1.83431579e-02 -5.64002994e-01 -5.91206470e-02 -9.95632332e-02\n",
      "    5.53769090e-01  1.69387207e-01]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-3.62695012e-02 -1.40138467e-02  5.12591358e-02  5.77254650e-04\n",
      "   -1.07948841e+00  6.63688761e-01]\n",
      "  [-1.19466800e-01 -1.97644521e-02  2.06756033e-01 -7.78370559e-04\n",
      "    1.77356777e+00 -7.79380815e-01]\n",
      "  [-2.09781253e-01 -3.99692675e-02 -6.49466774e-01 -3.11657796e-02\n",
      "   -7.69765458e-01 -1.13099257e-01]\n",
      "  ...\n",
      "  [-1.08585221e-01 -5.37907694e-03 -1.19911677e+00 -3.47714047e-03\n",
      "   -3.06061467e-01 -1.40085508e+00]\n",
      "  [-2.10575144e-01  2.31360009e-01 -1.10685922e+00  1.90111420e-01\n",
      "   -1.04102867e+00 -7.92958465e-01]\n",
      "  [ 1.48737401e-01 -6.87195110e-03  1.15739831e+00 -5.27118953e-03\n",
      "   -4.82879562e-01  5.32377995e-01]]\n",
      "\n",
      " [[ 5.16908106e-01  6.28407985e-01  6.53067086e-01  2.98071872e-01\n",
      "   -6.83469672e-01 -2.10023000e-01]\n",
      "  [-5.13932071e-01 -9.00102080e-01 -1.83045786e+00 -9.21874186e-01\n",
      "   -1.15879333e+00  9.20951461e-01]\n",
      "  [ 1.80288123e+00 -8.57664768e-02  1.58518410e+00 -8.89219561e-03\n",
      "   -8.27816542e-02 -1.95375734e+00]\n",
      "  ...\n",
      "  [-1.31145881e-01 -1.91833580e+00 -5.86567487e-01 -8.76298681e-01\n",
      "   -1.63399402e+00 -1.74640637e-01]\n",
      "  [-1.32938253e-01 -9.91358956e-01 -5.21413052e-01 -4.33035069e-01\n",
      "   -1.60440293e+00 -5.17812784e-01]\n",
      "  [ 3.13167206e-02 -5.90466398e-03  5.21304121e-01 -4.43864071e-03\n",
      "    1.22936719e+00 -1.61096441e+00]]\n",
      "\n",
      " [[ 3.32632014e-01  1.20804863e-02  1.13943704e+00  4.84287663e-02\n",
      "    3.27309162e-02  3.26528191e-01]\n",
      "  [-1.04739741e+00  1.80237481e-01 -6.83739738e-01  2.33591511e-02\n",
      "    4.80307258e-01 -2.07856940e+00]\n",
      "  [ 2.39346145e+00  4.62577968e-01  1.07325776e-01  2.18398107e-03\n",
      "    2.52672133e-01  7.28219278e-01]\n",
      "  ...\n",
      "  [ 1.33807865e-01 -4.79658215e-03  7.17626363e-01 -3.10207249e-03\n",
      "   -2.50736283e-01 -2.20994567e+00]\n",
      "  [ 8.57856976e-02 -2.45017607e-03  7.75154638e-01  7.00519535e-04\n",
      "    7.05770683e-01 -1.84298404e+00]\n",
      "  [-1.28749408e-01 -4.05974914e-01 -7.52069007e-01 -3.84521692e-01\n",
      "    8.99612147e-02  2.47970142e-01]]]\n"
     ]
    }
   ],
   "source": [
    "print(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fefb55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Activation, BatchNormalization, LSTM, Masking, Input, GRU\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68a94bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcdad66",
   "metadata": {},
   "source": [
    "GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3996078",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grumodel(max_len, n_var, rec_units, ndense=[10], l1_reg=0,\n",
    "              l2_reg=0, rec_act='sigmoid', extra_lab='none', rec_kernel_init='VarianceScaling',\n",
    "             dense_kernel_init='lecun_uniform', domask=False):\n",
    "    \n",
    "    rec_layer = 'GRU'\n",
    "    \n",
    "    track_inputs = Input(shape=(max_len, n_var,))\n",
    "    \n",
    "    if domask:\n",
    "        hidden = Masking( mask_value=0, name=\"masking_1\")(track_inputs)\n",
    "    else:\n",
    "        hidden = track_inputs\n",
    "    \n",
    "\n",
    "    if l1_reg > 1e-6 and l2_reg > 1e-6:\n",
    "        hidden = GRU(units=rec_units,\n",
    "                  kernel_initializer = rec_kernel_init, \n",
    "                  kernel_regularizer = regularizers.l1_l2(l1 = l1_reg, l2 = l2_reg),\n",
    "                  name = 'gru_l1l2')(hidden)\n",
    "    elif l1_reg > 1e-6:\n",
    "        hidden = GRU(units=rec_units,\n",
    "                  recurrent_activation = rec_act,\n",
    "                  kernel_initializer = rec_kernel_init, \n",
    "                  kernel_regularizer = regularizers.l1(l1 = l1_reg),\n",
    "                  name = 'gru_l1')(hidden)\n",
    "    elif l2_reg > 1e-6:\n",
    "        hidden = GRU(units=rec_units,\n",
    "                  recurrent_activation = rec_act,\n",
    "                  kernel_initializer = rec_kernel_init, \n",
    "                  kernel_regularizer = regularizers.l2(l2 = l2_reg),\n",
    "                  name = 'gru_l2')(hidden)\n",
    "    else:\n",
    "        hidden = GRU(units=rec_units,\n",
    "                  recurrent_activation = rec_act,\n",
    "                  kernel_initializer = rec_kernel_init, \n",
    "                  name = 'gru')(hidden)\n",
    "            \n",
    "\n",
    "    for ind,nd in enumerate(ndense):\n",
    "        hidden = Dense(nd, activation='relu', kernel_initializer=dense_kernel_init, name=f'dense_{ind}' )(hidden)\n",
    "    \n",
    "    output = Dense(3, activation='softmax', kernel_initializer=dense_kernel_init, name = 'output_softmax')(hidden)\n",
    "    \n",
    "    model = Model(inputs=track_inputs, outputs=output)\n",
    "    \n",
    "    d_layers = ''.join([ str(dl) for dl in ndense ])\n",
    "        \n",
    "    if domask:\n",
    "        mname  = f'MASKED_rnn_{rec_layer}.{rec_units}_Dense.{d_layers}_'\n",
    "    else:\n",
    "        mname  = f'rnn_{rec_layer}.{rec_units}_Dense.{d_layers}_'\n",
    "    mname += f'LSTMKernelInit.{rec_kernel_init}_DenseKernelInit.{dense_kernel_init}'\n",
    "    mname += f'KRl1.{l1_reg}_KRl2.{l2_reg}_recAct.{rec_act}' #LSTM kernel regularizer\n",
    "    \n",
    "    if 'none' not in extra_lab:\n",
    "        mname += f'_{extra_lab}'\n",
    "    \n",
    "    return model, mname\n",
    "\n",
    "#     mask = Masking( mask_value=0, name=\"masking_1\")(track_inputs)\n",
    "##########################################\n",
    "#                   use_bias=False,\n",
    "#                   activation='relu',\n",
    "#                   recurrent_activation='relu',\n",
    "#                   kernel_regularizer = regularizers.l1_l2(l1= 0.001, l2 = 0.0001), \n",
    "#                   bias_regularizer = regularizers.l1_l2(l1= 1, l2 = 1), \n",
    "#                   activity_regularizer=regularizers.l1_l2(l1= 0.001, l2 = 0.0001),\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869a7890",
   "metadata": {},
   "source": [
    "QGRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3626066e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import qkeras\n",
    "from qkeras import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2de39f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qgrumodel(max_len, n_var, rec_units, ndense=[10], l1_reg=0,\n",
    "              l2_reg=0, rec_act='sigmoid', extra_lab='none', rec_kernel_init='VarianceScaling',\n",
    "             dense_kernel_init='lecun_uniform', domask=False, quantization_bits=18, int_bits = 6):\n",
    "    \n",
    "    rec_layer = 'QGRU'\n",
    "    \n",
    "    track_inputs = Input(shape=(max_len, n_var,))\n",
    "    \n",
    "    if domask:\n",
    "        hidden = Masking( mask_value=0, name=\"masking_1\")(track_inputs)\n",
    "    else:\n",
    "        hidden = track_inputs\n",
    "    \n",
    "\n",
    "    if l1_reg > 1e-6 and l2_reg > 1e-6:\n",
    "        hidden = QGRU(units=rec_units,\n",
    "                  kernel_initializer = rec_kernel_init, \n",
    "                  kernel_regularizer = regularizers.l1_l2(l1 = l1_reg, l2 = l2_reg),\n",
    "#                   kernel_quantizer=quantized_bits(quantization_bits,int_bits,alpha=1),\n",
    "#                   recurrent_quantizer=quantized_bits(quantization_bits,int_bits,alpha=1),\n",
    "#                   bias_quantizer=quantized_bits(quantization_bits,int_bits,alpha=1),\n",
    "#                   state_quantizer=quantized_bits(quantization_bits,int_bits,alpha=1),\n",
    "                  kernel_quantizer=quantized_bits(quantization_bits,int_bits,1),\n",
    "                  recurrent_quantizer=quantized_bits(quantization_bits,int_bits,1),\n",
    "                  bias_quantizer=quantized_bits(quantization_bits,int_bits,1),\n",
    "                  state_quantizer=quantized_bits(quantization_bits,int_bits,1),\n",
    "                  name = 'qgru_l1l2')(hidden)\n",
    "    elif l1_reg > 1e-6:\n",
    "        hidden = QGRU(units=rec_units,\n",
    "                  recurrent_activation = rec_act,\n",
    "                  kernel_initializer = rec_kernel_init, \n",
    "                  kernel_regularizer = regularizers.l1(l1 = l1_reg),\n",
    "#                   kernel_quantizer=quantized_bits(quantization_bits,int_bits,alpha=1),\n",
    "#                   recurrent_quantizer=quantized_bits(quantization_bits,int_bits,alpha=1),\n",
    "#                   bias_quantizer=quantized_bits(quantization_bits,int_bits,alpha=1),\n",
    "#                   state_quantizer=quantized_bits(quantization_bits,int_bits,alpha=1),\n",
    "                  kernel_quantizer=quantized_bits(quantization_bits,int_bits,1),\n",
    "                  recurrent_quantizer=quantized_bits(quantization_bits,int_bits,1),\n",
    "                  bias_quantizer=quantized_bits(quantization_bits,int_bits,1),\n",
    "                  state_quantizer=quantized_bits(quantization_bits,int_bits,1),\n",
    "                  name = 'qgru_l1')(hidden)\n",
    "    elif l2_reg > 1e-6:\n",
    "        hidden = QGRU(units=rec_units,\n",
    "                  recurrent_activation = rec_act,\n",
    "                  kernel_initializer = rec_kernel_init, \n",
    "                  kernel_regularizer = regularizers.l2(l2 = l2_reg),\n",
    "#                   kernel_quantizer=quantized_bits(quantization_bits,int_bits,alpha=1),\n",
    "#                   recurrent_quantizer=quantized_bits(quantization_bits,int_bits,alpha=1),\n",
    "#                   bias_quantizer=quantized_bits(quantization_bits,int_bits,alpha=1),\n",
    "#                   state_quantizer=quantized_bits(quantization_bits,int_bits,alpha=1),\n",
    "                  kernel_quantizer=quantized_bits(quantization_bits,int_bits,1),\n",
    "                  recurrent_quantizer=quantized_bits(quantization_bits,int_bits,1),\n",
    "                  bias_quantizer=quantized_bits(quantization_bits,int_bits,1),\n",
    "                  state_quantizer=quantized_bits(quantization_bits,int_bits,1),\n",
    "                  name = 'qgru_l2')(hidden)\n",
    "    else:\n",
    "        hidden = QGRU(units=rec_units,\n",
    "                  recurrent_activation = rec_act,\n",
    "                  kernel_initializer = rec_kernel_init,\n",
    "#                   kernel_quantizer=quantized_bits(quantization_bits,int_bits,alpha=1),\n",
    "#                   recurrent_quantizer=quantized_bits(quantization_bits,int_bits,alpha=1),\n",
    "#                   bias_quantizer=quantized_bits(quantization_bits,int_bits,alpha=1),\n",
    "#                   state_quantizer=quantized_bits(quantization_bits,int_bits,alpha=1),\n",
    "                  kernel_quantizer=quantized_bits(quantization_bits,int_bits,1),\n",
    "                  recurrent_quantizer=quantized_bits(quantization_bits,int_bits,1),\n",
    "                  bias_quantizer=quantized_bits(quantization_bits,int_bits,1),\n",
    "                  state_quantizer=quantized_bits(quantization_bits,int_bits,1),\n",
    "                  name = 'qgru')(hidden)\n",
    "            \n",
    "\n",
    "    for ind,nd in enumerate(ndense):\n",
    "        hidden = QDense(nd, activation='relu',\n",
    "                        kernel_initializer=dense_kernel_init,\n",
    "#                         kernel_quantizer=quantized_bits(quantization_bits,int_bits,alpha=1),\n",
    "#                         bias_quantizer=quantized_bits(quantization_bits,int_bits,alpha=1),\n",
    "                        kernel_quantizer=quantized_bits(quantization_bits,int_bits,1),\n",
    "                        bias_quantizer=quantized_bits(quantization_bits,int_bits,1),\n",
    "                        name=f'dense_{ind}' )(hidden)\n",
    "    \n",
    "    output = QDense(3, activation='softmax',\n",
    "                    kernel_initializer=dense_kernel_init,\n",
    "#                     kernel_quantizer=quantized_bits(quantization_bits,int_bits,alpha=1),\n",
    "#                     bias_quantizer=quantized_bits(quantization_bits,int_bits,alpha=1),\n",
    "                    kernel_quantizer=quantized_bits(quantization_bits,int_bits,1),\n",
    "                    bias_quantizer=quantized_bits(quantization_bits,int_bits,1),\n",
    "                    name = 'output_softmax')(hidden)\n",
    "    \n",
    "    model = Model(inputs=track_inputs, outputs=output)\n",
    "    \n",
    "    d_layers = ''.join([ str(dl) for dl in ndense ])\n",
    "        \n",
    "    if domask:\n",
    "        mname  = f'MASKED_rnn_{rec_layer}.{rec_units}_Dense.{d_layers}_'\n",
    "    else:\n",
    "        mname  = f'rnn_{rec_layer}.{rec_units}_Dense.{d_layers}_'\n",
    "    mname += f'LSTMKernelInit.{rec_kernel_init}_DenseKernelInit.{dense_kernel_init}'\n",
    "    mname += f'KRl1.{l1_reg}_KRl2.{l2_reg}_recAct.{rec_act}' #LSTM kernel regularizer\n",
    "    \n",
    "    if 'none' not in extra_lab:\n",
    "        mname += f'_{extra_lab}'\n",
    "    \n",
    "    return model, mname\n",
    "\n",
    "#     mask = Masking( mask_value=0, name=\"masking_1\")(track_inputs)\n",
    "##########################################\n",
    "#                   use_bias=False,\n",
    "#                   activation='relu',\n",
    "#                   recurrent_activation='relu',\n",
    "#                   kernel_regularizer = regularizers.l1_l2(l1= 0.001, l2 = 0.0001), \n",
    "#                   bias_regularizer = regularizers.l1_l2(l1= 1, l2 = 1), \n",
    "#                   activity_regularizer=regularizers.l1_l2(l1= 0.001, l2 = 0.0001),\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb1c3d5",
   "metadata": {},
   "source": [
    "LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4448fcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstmmodel(max_len, n_var, rec_units, ndense=[10], l1_reg=0,\n",
    "              l2_reg=0, rec_act='sigmoid', extra_lab='none', rec_kernel_init='VarianceScaling',\n",
    "             dense_kernel_init='lecun_uniform', domask=False):\n",
    "    \n",
    "    rec_layer = 'LSTM'\n",
    "    \n",
    "    track_inputs = Input(shape=(max_len, n_var,))\n",
    "    \n",
    "    if domask:\n",
    "        hidden = Masking( mask_value=0, name=\"masking_1\")(track_inputs)\n",
    "    else:\n",
    "        hidden = track_inputs\n",
    "    \n",
    "    if l1_reg > 1e-6 and l2_reg > 1e-6:\n",
    "        hidden = LSTM(units=rec_units,\n",
    "                  recurrent_activation = rec_act,\n",
    "                  kernel_initializer = rec_kernel_init, \n",
    "                  kernel_regularizer = regularizers.l1_l2(l1 = l1_reg, l2 = l2_reg),\n",
    "                  name = 'lstm1_l1l2')(hidden)\n",
    "    elif l1_reg > 1e-6:\n",
    "        hidden = LSTM(units=rec_units,\n",
    "                  recurrent_activation = rec_act,\n",
    "                  kernel_initializer = rec_kernel_init, \n",
    "                  kernel_regularizer = regularizers.l1(l1 = l1_reg),\n",
    "                  name = 'lstm1_l1')(hidden)\n",
    "    elif l2_reg > 1e-6:\n",
    "        hidden = LSTM(units=rec_units,\n",
    "                  recurrent_activation = rec_act,\n",
    "                  kernel_initializer = rec_kernel_init, \n",
    "                  kernel_regularizer = regularizers.l2(l2 = l2_reg),\n",
    "                  name = 'lstm1_l2')(hidden)\n",
    "    else:\n",
    "        hidden = LSTM(units=rec_units,\n",
    "                  recurrent_activation = rec_act,\n",
    "                  kernel_initializer = rec_kernel_init, \n",
    "                  name = 'lstm1')(hidden)\n",
    "\n",
    "    for ind,nd in enumerate(ndense):\n",
    "        hidden = Dense(nd, activation='relu', kernel_initializer=dense_kernel_init, name=f'dense_{ind}' )(hidden)\n",
    "    \n",
    "    output = Dense(3, activation='softmax', kernel_initializer=dense_kernel_init, name = 'output_softmax')(hidden)\n",
    "    \n",
    "    model = Model(inputs=track_inputs, outputs=output)\n",
    "    \n",
    "    d_layers = ''.join([ str(dl) for dl in ndense ])\n",
    "        \n",
    "    if domask:\n",
    "        mname  = f'MASKED_rnn_{rec_layer}.{rec_units}_Dense.{d_layers}_'\n",
    "    else:\n",
    "        mname  = f'rnn_{rec_layer}.{rec_units}_Dense.{d_layers}_'\n",
    "    mname += f'LSTMKernelInit.{rec_kernel_init}_DenseKernelInit.{dense_kernel_init}'\n",
    "    mname += f'KRl1.{l1_reg}_KRl2.{l2_reg}_recAct.{rec_act}' #LSTM kernel regularizer\n",
    "    \n",
    "    if 'none' not in extra_lab:\n",
    "        mname += f'_{extra_lab}'\n",
    "    \n",
    "    return model, mname\n",
    "\n",
    "#     mask = Masking( mask_value=0, name=\"masking_1\")(track_inputs)\n",
    "##########################################\n",
    "#                   use_bias=False,\n",
    "#                   activation='relu',\n",
    "#                   recurrent_activation='relu',\n",
    "#                   kernel_regularizer = regularizers.l1_l2(l1= 0.001, l2 = 0.0001), \n",
    "#                   bias_regularizer = regularizers.l1_l2(l1= 1, l2 = 1), \n",
    "#                   activity_regularizer=regularizers.l1_l2(l1= 0.001, l2 = 0.0001),\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbb95e6",
   "metadata": {},
   "source": [
    "QLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eeb1051c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qlstmmodel(max_len, n_var, rec_units, ndense=[10], l1_reg=0,\n",
    "              l2_reg=0, rec_act='sigmoid', extra_lab='none', rec_kernel_init='VarianceScaling',\n",
    "             dense_kernel_init='lecun_uniform', domask=False, quantization_bits=0, int_bits =0):\n",
    "    \n",
    "    rec_layer = 'QLSTM'\n",
    "    \n",
    "    track_inputs = Input(shape=(max_len, n_var,))\n",
    "    \n",
    "    if domask:\n",
    "        hidden = Masking( mask_value=0, name=\"masking_1\")(track_inputs)\n",
    "    else:\n",
    "        hidden = track_inputs\n",
    "    \n",
    "    if l1_reg > 1e-6 and l2_reg > 1e-6:\n",
    "        hidden = QLSTM(units=rec_units,\n",
    "                  recurrent_activation = rec_act,\n",
    "                  kernel_initializer = rec_kernel_init, \n",
    "                  kernel_regularizer = regularizers.l1_l2(l1 = l1_reg, l2 = l2_reg),\n",
    "                  kernel_quantizer=quantized_bits(quantization_bits,int_bits,1),\n",
    "                  recurrent_quantizer=quantized_bits(quantization_bits,int_bits,1),\n",
    "                  bias_quantizer=quantized_bits(quantization_bits,int_bits,1),\n",
    "                  state_quantizer=quantized_bits(quantization_bits,int_bits,1),\n",
    "                  name = 'qlstm1_l1l2')(hidden)\n",
    "    elif l1_reg > 1e-6:\n",
    "        hidden = QLSTM(units=rec_units,\n",
    "                  recurrent_activation = rec_act,\n",
    "                  kernel_initializer = rec_kernel_init, \n",
    "                  kernel_regularizer = regularizers.l1(l1 = l1_reg),\n",
    "                  kernel_quantizer=quantized_bits(quantization_bits,int_bits,1),\n",
    "                  recurrent_quantizer=quantized_bits(quantization_bits,int_bits,1),\n",
    "                  bias_quantizer=quantized_bits(quantization_bits,int_bits,1),\n",
    "                  state_quantizer=quantized_bits(quantization_bits,int_bits,1),\n",
    "                  name = 'qlstm1_l1')(hidden)\n",
    "    elif l2_reg > 1e-6:\n",
    "        hidden = QLSTM(units=rec_units,\n",
    "                  recurrent_activation = rec_act,\n",
    "                  kernel_initializer = rec_kernel_init, \n",
    "                  kernel_regularizer = regularizers.l2(l2 = l2_reg),\n",
    "                  kernel_quantizer=quantized_bits(quantization_bits,int_bits,1),\n",
    "                  recurrent_quantizer=quantized_bits(quantization_bits,int_bits,1),\n",
    "                  bias_quantizer=quantized_bits(quantization_bits,int_bits,1),\n",
    "                  state_quantizer=quantized_bits(quantization_bits,int_bits,1),\n",
    "                  name = 'qlstm1_l2')(hidden)\n",
    "    else:\n",
    "        hidden = QLSTM(units=rec_units,\n",
    "                  recurrent_activation = rec_act,\n",
    "                  kernel_initializer = rec_kernel_init, \n",
    "                  kernel_quantizer=quantized_bits(quantization_bits,int_bits,1),\n",
    "                  recurrent_quantizer=quantized_bits(quantization_bits,int_bits,1),\n",
    "                  bias_quantizer=quantized_bits(quantization_bits,int_bits,1),\n",
    "                  state_quantizer=quantized_bits(quantization_bits,int_bits,1),\n",
    "                  name = 'qlstm1')(hidden)\n",
    "\n",
    "    for ind,nd in enumerate(ndense):\n",
    "        hidden = QDense(nd, activation='relu',\n",
    "                        kernel_quantizer=quantized_bits(quantization_bits,int_bits,1),\n",
    "                        bias_quantizer=quantized_bits(quantization_bits,int_bits,1),\n",
    "                        kernel_initializer=dense_kernel_init, name=f'dense_{ind}' )(hidden)\n",
    "    \n",
    "    output = QDense(3, activation='softmax',\n",
    "                    kernel_quantizer=quantized_bits(quantization_bits,int_bits,1),\n",
    "                    bias_quantizer=quantized_bits(quantization_bits,int_bits,1),\n",
    "                    kernel_initializer=dense_kernel_init, name = 'output_softmax')(hidden)\n",
    "    \n",
    "    model = Model(inputs=track_inputs, outputs=output)\n",
    "    \n",
    "    d_layers = ''.join([ str(dl) for dl in ndense ])\n",
    "        \n",
    "    if domask:\n",
    "        mname  = f'MASKED_rnn_{rec_layer}.{rec_units}_Dense.{d_layers}_'\n",
    "    else:\n",
    "        mname  = f'rnn_{rec_layer}.{rec_units}_Dense.{d_layers}_'\n",
    "    mname += f'LSTMKernelInit.{rec_kernel_init}_DenseKernelInit.{dense_kernel_init}'\n",
    "    mname += f'KRl1.{l1_reg}_KRl2.{l2_reg}_recAct.{rec_act}' #LSTM kernel regularizer\n",
    "    \n",
    "    if 'none' not in extra_lab:\n",
    "        mname += f'_{extra_lab}'\n",
    "    \n",
    "    return model, mname\n",
    "\n",
    "#     mask = Masking( mask_value=0, name=\"masking_1\")(track_inputs)\n",
    "##########################################\n",
    "#                   use_bias=False,\n",
    "#                   activation='relu',\n",
    "#                   recurrent_activation='relu',\n",
    "#                   kernel_regularizer = regularizers.l1_l2(l1= 0.001, l2 = 0.0001), \n",
    "#                   bias_regularizer = regularizers.l1_l2(l1= 1, l2 = 1), \n",
    "#                   activity_regularizer=regularizers.l1_l2(l1= 0.001, l2 = 0.0001),\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e54470b",
   "metadata": {},
   "source": [
    "Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a284db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "721b6e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.array( f5['x_test'] )\n",
    "y_test = to_categorical ( np.array( f5['y_test'] ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1595c1e2",
   "metadata": {},
   "source": [
    "Check Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01745651",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-04 12:51:25.219382: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-08-04 12:51:25.220084: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-08-04 12:51:25.220181: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (DESKTOP-CHSBCRQ): /proc/driver/nvidia/version does not exist\n",
      "2022-08-04 12:51:25.222594: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 15, 6)]           0         \n",
      "                                                                 \n",
      " qlstm1 (QLSTM)              (None, 120)               60960     \n",
      "                                                                 \n",
      " dense_0 (QDense)            (None, 50)                6050      \n",
      "                                                                 \n",
      " dense_1 (QDense)            (None, 10)                510       \n",
      "                                                                 \n",
      " output_softmax (QDense)     (None, 3)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 67,553\n",
      "Trainable params: 67,553\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "l1_reg = 0\n",
    "l2_reg = 0\n",
    "\n",
    "## GRU Model\n",
    "#model, model_name = grumodel(15, 6, 120, [50, 10],l1_reg=l1_reg, l2_reg=l2_reg)\n",
    "\n",
    "## LSTM Model\n",
    "#model, model_name = lstmmodel(15, 6, 120, [50, 10],l1_reg=l1_reg, l2_reg=l2_reg)\n",
    "\n",
    "## QGRU Model\n",
    "#model, model_name = qgrumodel(15, 6, 120, [50, 10],l1_reg=l1_reg, l2_reg=l2_reg, quantization_bits=4, int_bits = 2)\n",
    "\n",
    "## QLSTM Model\n",
    "model, model_name = qlstmmodel(15, 6, 120, [50, 10],l1_reg=l1_reg, l2_reg=l2_reg, quantization_bits=4, int_bits = 2)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d8676fc3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i= 15\n",
      "Epoch 1/15\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.8693 - accuracy: 0.6175\n",
      "Epoch 1: val_accuracy improved from -inf to 0.68173, saving model to epoch_0807_2int2fra/model_15epoch_weights.h5\n",
      "39/39 [==============================] - 77s 2s/step - loss: 0.8693 - accuracy: 0.6175 - val_loss: 0.7847 - val_accuracy: 0.6817\n",
      "Epoch 2/15\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.7661 - accuracy: 0.6886\n",
      "Epoch 2: val_accuracy improved from 0.68173 to 0.69804, saving model to epoch_0807_2int2fra/model_15epoch_weights.h5\n",
      "39/39 [==============================] - 58s 1s/step - loss: 0.7661 - accuracy: 0.6886 - val_loss: 0.7474 - val_accuracy: 0.6980\n",
      "Epoch 3/15\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.7369 - accuracy: 0.7036\n",
      "Epoch 3: val_accuracy improved from 0.69804 to 0.70990, saving model to epoch_0807_2int2fra/model_15epoch_weights.h5\n",
      "39/39 [==============================] - 58s 1s/step - loss: 0.7369 - accuracy: 0.7036 - val_loss: 0.7291 - val_accuracy: 0.7099\n",
      "Epoch 4/15\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.7202 - accuracy: 0.7122\n",
      "Epoch 4: val_accuracy improved from 0.70990 to 0.71391, saving model to epoch_0807_2int2fra/model_15epoch_weights.h5\n",
      "39/39 [==============================] - 58s 1s/step - loss: 0.7202 - accuracy: 0.7122 - val_loss: 0.7185 - val_accuracy: 0.7139\n",
      "Epoch 5/15\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.7115 - accuracy: 0.7161\n",
      "Epoch 5: val_accuracy improved from 0.71391 to 0.71603, saving model to epoch_0807_2int2fra/model_15epoch_weights.h5\n",
      "39/39 [==============================] - 57s 1s/step - loss: 0.7115 - accuracy: 0.7161 - val_loss: 0.7111 - val_accuracy: 0.7160\n",
      "Epoch 6/15\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.7060 - accuracy: 0.7187\n",
      "Epoch 6: val_accuracy improved from 0.71603 to 0.71888, saving model to epoch_0807_2int2fra/model_15epoch_weights.h5\n",
      "39/39 [==============================] - 58s 1s/step - loss: 0.7060 - accuracy: 0.7187 - val_loss: 0.7069 - val_accuracy: 0.7189\n",
      "Epoch 7/15\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.7035 - accuracy: 0.7194\n",
      "Epoch 7: val_accuracy did not improve from 0.71888\n",
      "39/39 [==============================] - 57s 1s/step - loss: 0.7035 - accuracy: 0.7194 - val_loss: 0.7078 - val_accuracy: 0.7181\n",
      "Epoch 8/15\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.6996 - accuracy: 0.7216\n",
      "Epoch 8: val_accuracy did not improve from 0.71888\n",
      "39/39 [==============================] - 56s 1s/step - loss: 0.6996 - accuracy: 0.7216 - val_loss: 0.7054 - val_accuracy: 0.7181\n",
      "Epoch 9/15\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.6970 - accuracy: 0.7226\n",
      "Epoch 9: val_accuracy improved from 0.71888 to 0.72118, saving model to epoch_0807_2int2fra/model_15epoch_weights.h5\n",
      "39/39 [==============================] - 56s 1s/step - loss: 0.6970 - accuracy: 0.7226 - val_loss: 0.7025 - val_accuracy: 0.7212\n",
      "Epoch 10/15\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.6926 - accuracy: 0.7248\n",
      "Epoch 10: val_accuracy improved from 0.72118 to 0.72170, saving model to epoch_0807_2int2fra/model_15epoch_weights.h5\n",
      "39/39 [==============================] - 58s 2s/step - loss: 0.6926 - accuracy: 0.7248 - val_loss: 0.6964 - val_accuracy: 0.7217\n",
      "Epoch 11/15\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.6921 - accuracy: 0.7253\n",
      "Epoch 11: val_accuracy did not improve from 0.72170\n",
      "39/39 [==============================] - 58s 1s/step - loss: 0.6921 - accuracy: 0.7253 - val_loss: 0.6994 - val_accuracy: 0.7211\n",
      "Epoch 12/15\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.6888 - accuracy: 0.7259\n",
      "Epoch 12: val_accuracy improved from 0.72170 to 0.72287, saving model to epoch_0807_2int2fra/model_15epoch_weights.h5\n",
      "39/39 [==============================] - 56s 1s/step - loss: 0.6888 - accuracy: 0.7259 - val_loss: 0.6950 - val_accuracy: 0.7229\n",
      "Epoch 13/15\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.6862 - accuracy: 0.7272\n",
      "Epoch 13: val_accuracy improved from 0.72287 to 0.72304, saving model to epoch_0807_2int2fra/model_15epoch_weights.h5\n",
      "39/39 [==============================] - 56s 1s/step - loss: 0.6862 - accuracy: 0.7272 - val_loss: 0.6962 - val_accuracy: 0.7230\n",
      "Epoch 14/15\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.6879 - accuracy: 0.7271\n",
      "Epoch 14: val_accuracy did not improve from 0.72304\n",
      "39/39 [==============================] - 55s 1s/step - loss: 0.6879 - accuracy: 0.7271 - val_loss: 0.6999 - val_accuracy: 0.7215\n",
      "Epoch 15/15\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.6869 - accuracy: 0.7273\n",
      "Epoch 15: val_accuracy improved from 0.72304 to 0.72500, saving model to epoch_0807_2int2fra/model_15epoch_weights.h5\n",
      "39/39 [==============================] - 57s 1s/step - loss: 0.6869 - accuracy: 0.7273 - val_loss: 0.6926 - val_accuracy: 0.7250\n",
      "294/294 [==============================] - 32s 101ms/step\n",
      "roc_score: 0.8149439884441061\n"
     ]
    }
   ],
   "source": [
    "roc = []\n",
    "for i in [2, 4, 6, 8, 10, 12, 14]:\n",
    "    print(\"i=\", i)\n",
    "    l1_reg = 0\n",
    "    l2_reg = 0\n",
    "\n",
    "    ## GRU Model\n",
    "    #model, model_name = grumodel(15, 6, 120, [50, 10], l1_reg=l1_reg, l2_reg=l2_reg)\n",
    "    \n",
    "    \n",
    "    ## QGRU Model\n",
    "    model, model_name = qgrumodel(15, 6, 120, [50, 10], l1_reg=l1_reg, l2_reg=l2_reg, quantization_bits=i+2, int_bits = 2)\n",
    "    \n",
    "    \n",
    "    ## LSTM Model\n",
    "    #model, model_name = lstmmodel(15, 6, 120, [50, 10], l1_reg=l1_reg, l2_reg=l2_reg)\n",
    "    \n",
    "    \n",
    "    # QLSTM Model\n",
    "    #model, model_name = qlstmmodel(15, 6, 120, [50, 10],l1_reg=l1_reg, l2_reg=l2_reg, quantization_bits=i, int_bits = 0)\n",
    "       \n",
    "    adam = Adam(learning_rate=0.01)\n",
    "    model.compile(optimizer=adam, loss=['categorical_crossentropy'], metrics=['accuracy'], weighted_metrics=[])\n",
    "    model_output = f'epoch_0807_2int2fra/model_{i}frac_weights.h5'\n",
    "    train = True\n",
    "    if train:\n",
    "        history = model.fit( x_train , y_train,\n",
    "                batch_size=2**14,\n",
    "                epochs=i,\n",
    "                validation_split=0.1,\n",
    "                shuffle = True,\n",
    "                sample_weight= w_train,\n",
    "                callbacks = [\n",
    "                    EarlyStopping(verbose=True, patience=15, monitor='val_accuracy'),\n",
    "                    ModelCheckpoint(model_output, monitor='val_accuracy', verbose=True, save_best_only=True)\n",
    "                    ],\n",
    "                verbose=True\n",
    "                )\n",
    "\n",
    "    model.load_weights(model_output)\n",
    "    pred_test = model.predict(x_test, batch_size=2**10)\n",
    "    roc_score = roc_auc_score(y_test, pred_test)\n",
    "    print(\"roc_score:\", roc_score)\n",
    "    roc.append(roc_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
